<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>On the Linguistic Representational Power of Neural Machine Translation Models - ACL Anthology</title><meta name=generator content="Hugo 0.68.3"><link href=/website/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/website/css/main.min.8976777c0832d068a49d330764e507857027f1efa3b8501cf349b0e2db7410fc.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/website/css/academicons.min.css><meta content="On the Linguistic Representational Power of Neural Machine Translation Models" name=citation_title><meta content="Yonatan Belinkov" name=citation_author><meta content="Computational Linguistics, Volume 46, Issue 1 - March 2020" name=citation_journal_title><meta content="46" name=citation_volume><meta content="1" name=citation_issue><meta content="2017" name=citation_publication_date><meta content="https://mobehbooei.github.io/website/2020.cl-1.1.pdf" name=citation_pdf_url><meta content="1" name=citation_firstpage><meta content="52" name=citation_lastpage><meta content="10.1162/coli_a_00367" name=citation_doi><meta property="og:title" content="On the Linguistic Representational Power of Neural Machine Translation Models"><meta property="og:image" content="https://mobehbooei.github.io/website/thumb/G17-2001.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.cl-1.1"><meta property="og:description" content="Yonatan Belinkov. Computational Linguistics, Volume 46, Issue 1 - March 2020. 2017."><link rel=canonical href=https://aclanthology.org/2020.cl-1.1></head><body><nav class="navbar navbar-expand-sm navbar-light bg-light bg-gradient-light shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/website/><img src=/website/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL Anthology</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/website/faq/>FAQ<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/website/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/website/info/contrib/>Submissions<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/website/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-primary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a href=https://mobehbooei.github.io/website/2020.cl-1.1.pdf>On the Linguistic Representational Power of Neural Machine Translation Models</a></h2><p class=lead><a href=/website/people/Y/Yonatan-Belinkov/>Yonatan Belinkov</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3"><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.</span></div></div><dl><dt>Anthology ID:</dt><dd>G17-2001</dd><dt>Volume:</dt><dd><a href=/website/volumes/G17-2/>Computational Linguistics, Volume 46, Issue 1 - March 2020</a></dd><dt>Month:</dt><dd></dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Cambridge, MA</dd><dt>Venue:</dt><dd><a href=/website/venues/gwf/>GWF</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>MIT Press</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>1–52</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.cl-1.1>https://aclanthology.org/2020.cl-1.1</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.1162/coli_a_00367 title="To the current version of the paper by DOI">10.1162/coli_a_00367</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">belinkov-etal-2020-linguistic</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Yonatan Belinkov. 2017. <a href=https://aclanthology.org/2020.cl-1.1>On the Linguistic Representational Power of Neural Machine Translation Models</a>. <i>Computational Linguistics, Volume 46, Issue 1 - March 2020</i>, 46(1):1–52.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.cl-1.1>On the Linguistic Representational Power of Neural Machine Translation Models</a> (Belinkov, GWF 2017)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeBibtexContent><i class="far fa-clipboard pr-2"></i>BibTeX</button>
<button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeModsContent><i class="far fa-clipboard pr-2"></i>MODS XML</button>
<button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeEndnoteContent><i class="far fa-clipboard pr-2"></i>Endnote</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://mobehbooei.github.io/website/2020.cl-1.1.pdf>https://mobehbooei.github.io/website/2020.cl-1.1.pdf</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/europarl>Europarl</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://mobehbooei.github.io/website/2020.cl-1.1.pdf title="Open PDF of 'On the Linguistic Representational Power of Neural Machine Translation Models'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" title="Open dialog for exporting citations" data-toggle=modal data-target=#citeModal href=#><i class="fas fa-quote-left"></i><span class=pl-2>Cite</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=On+the+Linguistic+Representational+Power+of+Neural+Machine+Translation+Models" title="Search for 'On the Linguistic Representational Power of Neural Machine Translation Models' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=true>BibTeX</a></li><li class=nav-item><a class=nav-link data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class=nav-link data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class=nav-link data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=false>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel><pre id=citeBibtexContent class="bg-light border p-2" style=max-height:50vh>@article{belinkov-etal-2020-linguistic,
    title = &#34;On the Linguistic Representational Power of Neural Machine Translation Models&#34;,
    author = &#34;Belinkov, Yonatan&#34;,
    journal = &#34;Computational Linguistics, Volume 46, Issue 1 - March 2020&#34;,
    volume = &#34;46&#34;,
    number = &#34;1&#34;,
    year = &#34;2017&#34;,
    address = &#34;Cambridge, MA&#34;,
    publisher = &#34;MIT Press&#34;,
    url = &#34;https://aclanthology.org/2020.cl-1.1&#34;,
    doi = &#34;10.1162/coli_a_00367&#34;,
    pages = &#34;1--52&#34;,
    abstract = &#34;Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.&#34;,
}
</pre><div class="modal-footer pb-1"><a class="btn btn-secondary" href=/website/G17-2001.bib><i class="fas fa-download pr-2"></i>Download as File</a>
<button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeBibtexContent><i class="far fa-clipboard pr-2"></i>Copy to Clipboard</button></div></div><div class=tab-pane id=citeMods role=tabpanel><pre id=citeModsContent class="bg-light border p-2" style=max-height:50vh>﻿&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt;
&lt;mods ID=&#34;belinkov-etal-2020-linguistic&#34;&gt;
    &lt;titleInfo&gt;
        &lt;title&gt;On the Linguistic Representational Power of Neural Machine Translation Models&lt;/title&gt;
    &lt;/titleInfo&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Yonatan&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Belinkov&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;originInfo&gt;
        &lt;dateIssued&gt;2017&lt;/dateIssued&gt;
    &lt;/originInfo&gt;
    &lt;typeOfResource&gt;text&lt;/typeOfResource&gt;
    &lt;genre authority=&#34;bibutilsgt&#34;&gt;journal article&lt;/genre&gt;
    &lt;relatedItem type=&#34;host&#34;&gt;
        &lt;titleInfo&gt;
            &lt;title&gt;Computational Linguistics, Volume 46, Issue 1 - March 2020&lt;/title&gt;
        &lt;/titleInfo&gt;
        &lt;originInfo&gt;
            &lt;issuance&gt;continuing&lt;/issuance&gt;
            &lt;publisher&gt;MIT Press&lt;/publisher&gt;
            &lt;place&gt;
                &lt;placeTerm type=&#34;text&#34;&gt;Cambridge, MA&lt;/placeTerm&gt;
            &lt;/place&gt;
        &lt;/originInfo&gt;
        &lt;genre authority=&#34;marcgt&#34;&gt;periodical&lt;/genre&gt;
        &lt;genre authority=&#34;bibutilsgt&#34;&gt;academic journal&lt;/genre&gt;
    &lt;/relatedItem&gt;
    &lt;abstract&gt;Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.&lt;/abstract&gt;
    &lt;identifier type=&#34;citekey&#34;&gt;belinkov-etal-2020-linguistic&lt;/identifier&gt;
    &lt;identifier type=&#34;doi&#34;&gt;10.1162/coli_a_00367&lt;/identifier&gt;
    &lt;location&gt;
        &lt;url&gt;https://aclanthology.org/2020.cl-1.1&lt;/url&gt;
    &lt;/location&gt;
    &lt;part&gt;
        &lt;date&gt;2017&lt;/date&gt;
        &lt;detail type=&#34;volume&#34;&gt;&lt;number&gt;46&lt;/number&gt;&lt;/detail&gt;
        &lt;detail type=&#34;issue&#34;&gt;&lt;number&gt;1&lt;/number&gt;&lt;/detail&gt;
        &lt;extent unit=&#34;page&#34;&gt;
            &lt;start&gt;1&lt;/start&gt;
            &lt;end&gt;52&lt;/end&gt;
        &lt;/extent&gt;
    &lt;/part&gt;
&lt;/mods&gt;
&lt;/modsCollection&gt;
</pre><div class="modal-footer pb-1"><a class="btn btn-secondary" href=/website/G17-2001.xml><i class="fas fa-download pr-2"></i>Download as File</a>
<button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeModsContent><i class="far fa-clipboard pr-2"></i>Copy to Clipboard</button></div></div><div class=tab-pane id=citeEndnote role=tabpanel><pre id=citeEndnoteContent class="bg-light border p-2" style=max-height:50vh>﻿%0 Journal Article
%T On the Linguistic Representational Power of Neural Machine Translation Models
%A Belinkov, Yonatan
%J Computational Linguistics, Volume 46, Issue 1 - March 2020
%D 2017
%V 46
%N 1
%I MIT Press
%C Cambridge, MA
%F belinkov-etal-2020-linguistic
%X Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.
%R 10.1162/coli_a_00367
%U https://aclanthology.org/2020.cl-1.1
%U https://doi.org/10.1162/coli_a_00367
%P 1-52

</pre><div class="modal-footer pb-1"><a class="btn btn-secondary" href=/website/G17-2001.endf><i class="fas fa-download pr-2"></i>Download as File</a>
<button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeEndnoteContent><i class="far fa-clipboard pr-2"></i>Copy to Clipboard</button></div></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[On the Linguistic Representational Power of Neural Machine Translation Models](https://aclanthology.org/2020.cl-1.1) (Belinkov, GWF 2017)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.cl-1.1>On the Linguistic Representational Power of Neural Machine Translation Models</a> (Belinkov, GWF 2017)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Yonatan Belinkov. 2017. <a href=https://aclanthology.org/2020.cl-1.1>On the Linguistic Representational Power of Neural Machine Translation Models</a>. <i>Computational Linguistics, Volume 46, Issue 1 - March 2020</i>, 46(1):1–52.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div></section></div><footer class="bg-gradient-light py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5"><div class=container><p class="text-muted small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="text-muted small px-1">The ACL Anthology is managed and built by the <a href=/website/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="text-muted small px-1"><i>Site last built on 21 November 2022 at 04:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/51de55f083dd8e08971d5f9d4a69e24f90b89cf2>commit 51de55f0</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script><script>$(function(){$('[data-toggle="tooltip"]').tooltip();if($("#toggle-all-abstracts")){$("#toggle-all-abstracts").click(function(){var target=$("#toggle-all-abstracts");target.attr("disabled",true);if(target.attr("data-toggle-state")=="hide"){$(".abstract-collapse").collapse('show');target.attr("data-toggle-state","show");}else{$(".abstract-collapse").collapse('hide');target.attr("data-toggle-state","hide");}
target.attr("disabled",false);});$("#toggle-all-abstracts").attr("disabled",false);}})</script><script src=/website/js/clipboard.min.js></script><script>$(document).ready(function(){if(ClipboardJS.isSupported()){success_fn=function(e){var src=$(e.trigger);src.toggleClass("btn-success");src.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check");e.clearSelection();setTimeout(function(){src.toggleClass("btn-success");src.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check");},2000);};var clipboard=new ClipboardJS(".btn-clipboard");clipboard.on('success',success_fn);$(".btn-clipboard").removeClass("d-none");var clipboard_outside=new ClipboardJS(".btn-clipboard-outside",{text:function(trigger){var target=trigger.getAttribute("data-clipboard-target");return $(target).text();}});clipboard_outside.on('success',success_fn);$(".btn-clipboard-outside").removeClass("d-none");}});</script></body></html>